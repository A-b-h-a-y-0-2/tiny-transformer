{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADPXZh-9npF6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "def to_2tuple(x):\n",
        "    \"\"\"\n",
        "    Ensure that the input x is a tuple.\n",
        "    If x is not a tuple, return a tuple with x repeated twice.\n",
        "    \"\"\"\n",
        "    if isinstance(x, tuple):\n",
        "        return x\n",
        "    return (x, x)\n",
        "\n",
        "def window_partition(x, window_size):\n",
        "    \"\"\"\n",
        "    Partition the input tensor x into non-overlapping windows of size window_size.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor with shape (B, H, W, C)\n",
        "        window_size: The size of the windows to partition into\n",
        "\n",
        "    Returns:\n",
        "        Tensor with shape (B, num_windows, window_size * window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "    x = x.view(B, H // window_size * W // window_size, window_size * window_size, C)\n",
        "    return x\n",
        "\n",
        "def window_reverse(windows, window_size, H, W):\n",
        "    \"\"\"\n",
        "    Reconstruct the original input tensor from the partitioned windows.\n",
        "\n",
        "    Args:\n",
        "        windows: Tensor with partitioned windows of shape (B, num_windows, window_size * window_size, C)\n",
        "        window_size: The size of the windows\n",
        "        H: Original height of the input tensor\n",
        "        W: Original width of the input tensor\n",
        "\n",
        "    Returns:\n",
        "        Reconstructed tensor with shape (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B, num_windows, window_size, C = windows.shape\n",
        "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, C)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
        "    x = x.view(B, H, W, C)\n",
        "    return x\n",
        "\n",
        "class RelativePositionalBias(nn.Module):\n",
        "    \"\"\"\n",
        "    Module for adding relative positional bias to the attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        window_size: The size of the windows\n",
        "        num_heads: Number of attention heads\n",
        "    \"\"\"\n",
        "    def __init__(self, window_size, num_heads):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        self.bias = nn.Parameter(torch.zeros(num_heads, window_size * 2 - 1, window_size * 2 - 1))\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention mechanism with optional relative positional bias.\n",
        "\n",
        "    Args:\n",
        "        dim: Input dimension of the features\n",
        "        num_heads: Number of attention heads\n",
        "        dropout: Dropout rate\n",
        "        window_size: Size of the windows (for relative positional bias)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, num_heads, dropout=0.0, window_size=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        assert head_dim * num_heads == dim, f\"dim {dim} must be divisible by num_heads {num_heads}\"\n",
        "\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        if window_size is not None:\n",
        "            self.relative_position_bias = RelativePositionalBias(window_size, num_heads)\n",
        "        else:\n",
        "            self.relative_position_bias = None\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, N, C)\n",
        "            mask: Optional mask tensor\n",
        "\n",
        "        Returns:\n",
        "            Output tensor with shape (B, N, C)\n",
        "        \"\"\"\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        q = q * self.scale\n",
        "\n",
        "        if self.relative_position_bias is not None:\n",
        "            attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "            attn_weights = attn_weights + self.relative_position_bias.bias\n",
        "            attn_weights = attn_weights.softmax(dim=-1)\n",
        "        else:\n",
        "            attn_weights = torch.matmul(q, k.transpose(-2, -1))\n",
        "            attn_weights = attn_weights.softmax(dim=-1)\n",
        "\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        x = torch.matmul(attn_weights, v)\n",
        "        x = x.transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class MSWFeatureFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Module for Multi-Scale Window (MSW) feature fusion.\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Embedding dimension of the input features\n",
        "        num_windows: Number of different window sizes to fuse\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_windows):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_windows = num_windows\n",
        "\n",
        "        # Linear projection layer for each window size\n",
        "        self.proj_layers = nn.ModuleList([nn.Linear(embed_dim, embed_dim) for _ in range(num_windows)])\n",
        "        # Fusion weight layer\n",
        "        self.fusion_weight = nn.Parameter(torch.ones(num_windows))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Combines features from different window sizes using trainable weights.\n",
        "\n",
        "        Args:\n",
        "            x: List of tensors, each containing features from a specific window size.\n",
        "\n",
        "        Returns:\n",
        "            Fused feature tensor with shape (B, N, C), where:\n",
        "                - B is the batch size\n",
        "                - N is the number of patches\n",
        "                - C is the embedding dimension\n",
        "        \"\"\"\n",
        "        assert len(x) == self.num_windows, \"Number of inputs doesn't match number of window sizes.\"\n",
        "\n",
        "        # Apply linear projection to each window feature\n",
        "        projected_features = [layer(feat) for layer, feat in zip(self.proj_layers, x)]\n",
        "        # Apply fusion weights and sum\n",
        "        weighted_features = [feat * w for feat, w in zip(projected_features, self.fusion_weight)]\n",
        "        fused_features = torch.sum(torch.stack(weighted_features), dim=0)\n",
        "\n",
        "        return fused_features\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    ECG Signal to Patch Embedding\n",
        "\n",
        "    Args:\n",
        "        signal_length (int): Length of each ECG channel signal. Default: 1000.\n",
        "        patch_size (int): Patch token size. Default: 5.\n",
        "        in_chans (int): Number of input channels (leads). Default: 12.\n",
        "        embed_dim (int): Number of linear projection output channels. Default: 512.\n",
        "        norm_layer (nn.Module, optional): Normalization layer. Default: None\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, signal_length=1000, patch_size=5, in_chans=12, embed_dim=512, norm_layer=None):\n",
        "        super().__init__()\n",
        "        self.signal_length = signal_length\n",
        "        self.patch_size = patch_size\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        self.num_patches = signal_length // patch_size\n",
        "        self.proj = nn.Conv1d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        if norm_layer is not None:\n",
        "            self.norm = norm_layer(embed_dim)\n",
        "        else:\n",
        "            self.norm = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the patch embedding layer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, C, L)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor with shape (B, num_patches, embed_dim)\n",
        "        \"\"\"\n",
        "        B, C, L = x.shape\n",
        "        assert L == self.signal_length, f\"Input signal length ({L}) doesn't match model ({self.signal_length}).\"\n",
        "        x = self.proj(x).transpose(1, 2)  # B, num_patches, embed_dim\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculate the number of floating point operations (FLOPs) for the patch embedding layer.\n",
        "\n",
        "        Returns:\n",
        "            Number of FLOPs.\n",
        "        \"\"\"\n",
        "        num_patches = self.signal_length // self.patch_size\n",
        "        flops = num_patches * self.embed_dim * self.in_chans * self.patch_size\n",
        "        if self.norm is not None:\n",
        "            flops += num_patches * self.embed_dim\n",
        "        return flops\n",
        "\n",
        "class MSWTransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Scale Window (MSW) Transformer Block.\n",
        "\n",
        "    Args:\n",
        "        embed_dim (int): Embedding dimension of the input features. Default: 512.\n",
        "        num_heads (int): Number of attention heads. Default: 8.\n",
        "        window_sizes (list): List of window sizes to use for multi-scale attention. Default: [5, 10, 15].\n",
        "        mlp_ratio (float): Ratio of MLP hidden dimension to embedding dimension. Default: 4.0.\n",
        "        dropout (float): Dropout rate. Default: 0.1.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim=512, num_heads=8, window_sizes=[5, 10, 15], mlp_ratio=4.0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.window_sizes = window_sizes\n",
        "        self.mlp_ratio = mlp_ratio\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        for window_size in window_sizes:\n",
        "            self.layers.append(\n",
        "                nn.ModuleList([\n",
        "                    nn.LayerNorm(embed_dim),\n",
        "                    Attention(embed_dim, num_heads, dropout=dropout, window_size=window_size),\n",
        "                    nn.LayerNorm(embed_dim),\n",
        "                    nn.Sequential(\n",
        "                        nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
        "                        nn.GELU(),\n",
        "                        nn.Dropout(dropout),\n",
        "                        nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
        "                        nn.Dropout(dropout),\n",
        "                    )\n",
        "                ])\n",
        "            )\n",
        "\n",
        "        # Feature Fusion module\n",
        "        self.feature_fusion = MSWFeatureFusion(embed_dim, len(window_sizes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MSW Transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor with shape (B, N, C)\n",
        "\n",
        "        Returns:\n",
        "            Output tensor with shape (B, N, C)\n",
        "        \"\"\"\n",
        "        B, N, C = x.shape\n",
        "        H = W = int(N ** 0.5)\n",
        "        x = x.view(B, H, W, C)\n",
        "\n",
        "        # Window partition\n",
        "        x_windows = []\n",
        "        for window_size in self.window_sizes:\n",
        "            x_windows.append(window_partition(x, window_size))\n",
        "\n",
        "        # Apply MSW-Transformer to each window\n",
        "        outputs = []\n",
        "        for i, (window_size, x_window) in enumerate(zip(self.window_sizes, x_windows)):\n",
        "            num_windows = x_window.shape[1]\n",
        "            x_window = x_window.view(-1, window_size * window_size, C)\n",
        "            for layer in self.layers[i]:\n",
        "                x_window = layer(x_window)\n",
        "            outputs.append(x_window.view(-1, num_windows, window_size * window_size, C))\n",
        "\n",
        "        # Flatten and concatenate window outputs\n",
        "        outputs = [o.view(B, H * W, -1) for o in outputs]\n",
        "        x = torch.cat(outputs, dim=-1)\n",
        "\n",
        "        # Perform feature fusion\n",
        "        x = self.feature_fusion([x[:, i::len(self.window_sizes), :] for i in range(len(self.window_sizes))])\n",
        "\n",
        "        return x\n",
        "\n",
        "    def flops(self):\n",
        "        \"\"\"\n",
        "        Calculate the number of floating point operations (FLOPs) for the MSW Transformer block.\n",
        "\n",
        "        Returns:\n",
        "            Number of FLOPs.\n",
        "        \"\"\"\n",
        "        flops = 0\n",
        "        for layer in self.layers:\n",
        "            ln1, attn, ln2, mlp = layer\n",
        "            flops += 2 * self.embed_dim * self.num_heads  # LayerNorm FLOPs\n",
        "            flops += attn.flops()  # Attention FLOPs\n",
        "            flops += 2 * self.embed_dim * int(self.embed_dim * self.mlp_ratio)  # MLP\n",
        "\n",
        "        # Add FLOPs for feature fusion\n",
        "        flops += self.feature_fusion.num_windows * (self.embed_dim + self.embed_dim * len(self.window_sizes))\n",
        "        return flops\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#(28/07/2024)\n",
        "import numpy as np,os,pandas as pd\n",
        "from tqdm import tqdm\n",
        "import wfdb \n",
        "from scipy.signal import resample\n",
        "def load_ptb_xl(path, sampling_rate = 100):\n",
        "    Y = pd.read_csv(os.path.join(path,'ptbxl_dataset.csv'),index_col = 'ecg_id')#PTB-XL dataset is loaded from a CSV file, the ecg_id is used as the index\n",
        "    Y.scp_codes = Y.scp_codes.apply(lambda x:eval(x)) # scp_codes column contains diagnostic information in string format, which is converted to a dictionary format using eval.\n",
        "    X = []\n",
        "    for i in tqdm(Y.index, desc = \"Loading ECG Signals\"):#progress bar\n",
        "        data, _ = wfdb.rdsamp(os.path.join(path,Y.loc[i].filename_hr))#For each record, the corresponding ECG signal data is read, file path for each signal is constructed using the filename_hr \n",
        "        X.append(data)\n",
        "    X = np.array(X)\n",
        "\n",
        "    if sampling_rate!=500:\n",
        "        X = resample(X,int(X.shape[1]*sampling_rate/500),axis = 1) #The resample function from scipy.signal is used to adjust the number of samples to match the new sampling rate. \n",
        "    \n",
        "    label_counts = Y.scp.apply(pd.Series).sum() #computes the frequency of each label\n",
        "    selected_labels = label_counts[label_counts>0.05 * len(Y)].index.tolist()#Labels that appear in more than 5% of the ECGs are selected\n",
        "    y = np.array(Y.scp_codes.apply(lambda x:[1 if label in x else 0 for label in selected_labels]).tolist()) #binary label matrix y is created where each entry is 1 if the label is present in the ECG record, and 0 otherwise.\n",
        "    return X,y,selected_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.4 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "d348431bd9d2768439cf04cc1df6af8f2d78efd620b62912fbae7173d0fee7da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
